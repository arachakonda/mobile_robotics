{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b522c33d",
   "metadata": {
    "id": "b522c33d"
   },
   "source": [
    "# Assignment 1: Transformations and Representations\n",
    "\n",
    "Roll number: \\<Roll number here\\>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7c7311",
   "metadata": {
    "id": "2c7c7311"
   },
   "source": [
    "# Instructions\n",
    "\n",
    "- Code must be written in Python in Jupyter Notebooks. We highly recommend using anaconda distribution or at the minimum, virtual environments for this assignment. \n",
    "- Save all your results in ```results/<question_number>/<sub_topic_number>/```\n",
    "- For this assignment, you will be using Open3D extensively. Refer to [Open3D Documentation](http://www.open3d.org/docs/release/): you can use the in-built methods and **unless explicitly mentioned**, don't need to code from scratch for this assignment. \n",
    "- Make sure your code is modular since you may need to reuse parts for future assignments.\n",
    "- Make sure any extra files you that you need to submit, place it in *'results'* folder.\n",
    "- Answer the descriptive questions in your own words with context & clarity. Do not copy answers from online resources or lecture notes.\n",
    "- The **deadline** for this assignment is on **23/08/2022 at 11:55pm**. Please note that there will be no extensions.\n",
    "- Plagiarism is **strictly prohibited**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6ccd58",
   "metadata": {
    "id": "bb6ccd58"
   },
   "source": [
    "# Submission Instructions\n",
    "\n",
    "1. Make sure your code runs without any errors after reinitializing the kernel and removing all saved variables.\n",
    "2. After completing your code and saving your results, zip the folder with name as ``<roll_number>_MR2022_<assignment_number>.zip``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6c57fe",
   "metadata": {
    "id": "3a6c57fe"
   },
   "source": [
    "## 1. 3D Data and Open3D\n",
    "\n",
    "1. Please find mesh files in **data/Q1** folder. Using these mesh files and your own creativity/visualisation, create a \"Table\" **pointcloud** scene. The table scene should be realistic, scaled appropiately. Use all the meshes given in the folder and treat them as objects kept on the table. \n",
    "\n",
    "    You are expected to perform different functions on the individual mesh files: first convert the mesh files to pointclouds and on each pointcloud perform operations such as scaling, rotation, translation. Next, visualize them together. The visualization should represent a pointcloud of a realistic table scene. Save the scene as **.pcd** file. \n",
    "\n",
    "    **Please do not copy as we may use your contribution to create a table top dataset.**\n",
    "\n",
    "    Refer below image for example of a table-top scene:\n",
    "\n",
    "    <img src=\"img/1.jpeg\"  width=\"500\" >\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "2. Use the final table scene pointcloud obtained from part 1. \n",
    "    - Use Open3D to generate partial pointclouds from different camera views (at least 4 views). This means, that you need to crop or capture the points in the pointcloud that are visible only from a given viewpoint. \n",
    "    - Using these partial pointclouds, you are now expected to generate the full scene pointcloud back by registering the pointclouds to a global frame. Save the partial and reconstructed pointclouds in different files. \n",
    "    - **[ BONUS ]** Finally, compute the error using \"Chamfer's Distance (CD)\" between the ground truth scene pointcloud and the reconstructed pointcloud. Perform an analysis: \n",
    "      1. Why is the CD not 0?\n",
    "      2. How does the CD change as the number of viewpoints increase / decrease? \n",
    "      3. Can we optimize the viewpoints (by hit and trial) such that the CD reduces?\n",
    "\n",
    "Refer the following link for solving Q1:\n",
    "- Hidden-Point-Removal Open3D API: http://www.open3d.org/docs/latest/tutorial/Basic/pointcloud.html#Hidden-point-removal\n",
    "\n",
    "- Chamfer's Distance:  https://pytorch3d.readthedocs.io/en/latest/modules/loss.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bc0071",
   "metadata": {},
   "source": [
    "### References for this question:\n",
    "* how to open obj file and save as ply file: http://www.open3d.org/docs/0.9.0/tutorial/Basic/file_io.html\n",
    "* how to convert obj or ply file to pcd file via sampling : http://www.open3d.org/docs/release/tutorial/geometry/mesh.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3174677",
   "metadata": {
    "id": "e3174677"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import open3d as o3d\n",
    "import os\n",
    "from scipy import stats as st\n",
    "\n",
    "data_path =\"data/Q1\"\n",
    "results_path =\"results/Q1/pointclouds\"\n",
    "table_path = \"results/Q1/pointclouds/table.pcd\"\n",
    "pcd_list = []\n",
    "\n",
    "def configure_environment():\n",
    "    o3d.utility.set_verbosity_level(o3d.utility.VerbosityLevel.Error)\n",
    "    np.set_printoptions(suppress=True)\n",
    "\n",
    "def read_mesh_write_pointcloud(data_path,results_path):\n",
    "    for file in os.listdir(data_path):\n",
    "        print(f\"{file}\")\n",
    "        mesh = o3d.io.read_triangle_mesh(f\"{data_path}/{file}\")\n",
    "        #print(mesh)  \n",
    "        pointcloud = mesh.sample_points_poisson_disk(number_of_points=1000)\n",
    "        o3d.io.write_point_cloud(f\"{results_path}/{os.path.splitext(file)[0]}.pcd\", pointcloud)\n",
    "        \n",
    "def read_table_bounds(pcd_list, path):\n",
    "    pcd = o3d.io.read_point_cloud(path)\n",
    "    R = pcd.get_rotation_matrix_from_xyz((-np.pi/2, 0, -np.pi/2))\n",
    "    pcd = pcd.rotate(R, center=pcd.get_center())\n",
    "    pcd.translate((0,0,0), relative=False)\n",
    "            \n",
    "    #segment plane        \n",
    "    plane_model, inliers = pcd.segment_plane(distance_threshold = 0.01, ransac_n = 3, num_iterations =1000)\n",
    "    [a, b, c, d] = plane_model\n",
    "            \n",
    "    inlier_cloud =pcd.select_by_index(inliers)\n",
    "    inlier_cloud.paint_uniform_color([1.0, 0, 0])\n",
    "    outlier_cloud = pcd.select_by_index(inliers,invert=True)\n",
    "    outlier_cloud.paint_uniform_color([1, 0.2, 0.2])\n",
    "            \n",
    "    #get min and max x of the inlier cloud\n",
    "    min_x = np.min(np.array(inlier_cloud.points), axis=0)[0]\n",
    "    max_x = np.max(np.array(inlier_cloud.points), axis=0)[0]\n",
    "    #get min and max z of the inlier cloud        \n",
    "    min_z = np.min(np.array(inlier_cloud.points), axis=0)[2]\n",
    "    max_z = np.max(np.array(inlier_cloud.points), axis=0)[2]\n",
    "            \n",
    "            \n",
    "    pcd_list.append(inlier_cloud)\n",
    "    pcd_list.append(outlier_cloud)\n",
    "    \n",
    "    return (min_x, max_x, min_z, max_z)\n",
    "\n",
    "def read_obj_bounds(pcd):\n",
    "    #get min and max x of the inlier cloud\n",
    "    min_x = np.min(np.array(pcd.points), axis=0)[0]\n",
    "    max_x = np.max(np.array(pcd.points), axis=0)[0]\n",
    "    #get min and max z of the inlier cloud        \n",
    "    min_z = np.min(np.array(pcd.points), axis=0)[2]\n",
    "    max_z = np.max(np.array(pcd.points), axis=0)[2]\n",
    "    \n",
    "\n",
    "def read_table_level(path):\n",
    "    pcd = o3d.io.read_point_cloud(path)\n",
    "    R = pcd.get_rotation_matrix_from_xyz((-np.pi/2, 0, -np.pi/2))\n",
    "    pcd = pcd.rotate(R, center=pcd.get_center())\n",
    "    pcd.translate((0,0,0), relative=False)\n",
    "            \n",
    "    #segment plane        \n",
    "    plane_model, inliers = pcd.segment_plane(distance_threshold = 0.01, ransac_n = 3, num_iterations =1000)\n",
    "    [a, b, c, d] = plane_model\n",
    "            \n",
    "    inlier_cloud =pcd.select_by_index(inliers)\n",
    "    bias = 1\n",
    "    table_surface = np.max(np.array(inlier_cloud.points), axis=0)[1]+bias\n",
    "    return table_surface    \n",
    "\n",
    "def sample_random_pos(table_bounds):\n",
    "    bias = 3\n",
    "    posx = np.random.randint(low=table_bounds[0]+bias, high=table_bounds[1]-bias)\n",
    "    posz = np.random.randint(low=table_bounds[2]+bias, high=table_bounds[3]-bias)\n",
    "    return (posx,posz)\n",
    "   \n",
    "    \n",
    "def viz_pcd(results_path,pcd_list, table_bounds, table_level):\n",
    "    #create a coordinate frame\n",
    "    obj_number = 0\n",
    "    world_coordinate_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=20, origin=[0, 0, 0])\n",
    "    #append the world coordinate frame to the list\n",
    "    pcd_list.append(world_coordinate_frame)\n",
    "    #iterate over all the files in the directory\n",
    "    for file in os.listdir(results_path):\n",
    "        obj_number+= 1\n",
    "        #print(f\"{file}\")\n",
    "        #read point clouds\n",
    "        pcd = o3d.io.read_point_cloud(f\"{results_path}/{file}\")\n",
    "        #if the point cloud is not table scale the parts\n",
    "        if(os.path.splitext(file)[0] != \"table\"):\n",
    "            posx, posz = sample_random_pos(table_bounds)\n",
    "            pcd = pcd.scale(7, center=pcd.get_center())\n",
    "            pcd.translate((posx,table_level, posz), relative=False)\n",
    "            print(f\"{os.path.splitext(file)[0]} center:{pcd.get_center()}\")\n",
    "            pcd.paint_uniform_color([0.111*obj_number, .143*obj_number, 1-0.023*obj_number])\n",
    "            pcd_list.append(pcd)\n",
    "        else:\n",
    "            print(f\"{os.path.splitext(file)[0]} center:{pcd.get_center()}\")     \n",
    "            \n",
    "    o3d.visualization.draw_geometries(pcd_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2312b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boat.obj\n",
      "trashcan.obj\n",
      "plane.obj\n",
      "laptop.obj\n",
      "table.obj\n",
      "car.obj\n",
      "4.517631504846264\n",
      "trashcan center:[7.        4.5176315 5.       ]\n",
      "car center:[-14.          4.5176315  -4.       ]\n",
      "table center:[89.11841909 90.74375466 25.6229687 ]\n",
      "laptop center:[ 4.         4.5176315 -3.       ]\n",
      "boat center:[-12.          4.5176315 -10.       ]\n",
      "plane center:[-11.          4.5176315   9.       ]\n"
     ]
    }
   ],
   "source": [
    "configure_environment()\n",
    "read_mesh_write_pointcloud(data_path,results_path)\n",
    "table_bounds = np.array(read_table_bounds(pcd_list, table_path), dtype=int)\n",
    "table_level = read_table_level(table_path)\n",
    "print(table_level)\n",
    "viz_pcd(results_path,pcd_list, table_bounds, table_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d688be4e",
   "metadata": {
    "id": "d688be4e"
   },
   "source": [
    "## 2. Euler Angles, Rotation Matrices, and Quaternions\n",
    "1. Write a function (do not use inbuilt libraries for this question):\n",
    "    - that returns a rotation matrix given the angles $\\alpha$, $\\beta$, and $\\gamma$ in radians (X-Y-Z).\n",
    "    - to convert a rotation matrix to quaternion and vice versa. \n",
    "\n",
    "2. What is a Gimbal lock? Suppose an airplane increases its pitch from $0°$ to $90°$. \n",
    "\n",
    "    - Let $R_{gmb\\beta}$ be the rotation matrix for $\\beta=90°$. Find $R_{gmb\\beta}$.\n",
    "    - Consider the point $p = [0, 1, 0]ᵀ $ on the pitched airplane, i.e. the tip of the wing. Does there exist any $α , γ$ such that $p_{new} = R_{gmb\\beta}\\; p$ for:\n",
    "      1. $p_{new} = [1, 0, 0]ᵀ $\n",
    "      2. For some  $p_{new}$ on the XY unit circle?\n",
    "      3. For some  $p_{new}$ on the YZ unit circle?\n",
    "      \n",
    "      Show your work for all three parts and briefly explain your reasoning. Why is $\\beta=90°$  a “certain problematic value”?\n",
    "\n",
    "    <img src=\"img/2.3.jpeg\"  width=\"500\" ><br>\n",
    "    \n",
    "    <img src=\"img/2.1.jpeg\"  width=\"500\" ><br>\n",
    "\n",
    "    <img src=\"img/2.2.jpeg\"  width=\"500\" >\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a3b89f6",
   "metadata": {
    "id": "4a3b89f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The entered euler angles are[alpha,beta,gamma]: [1.57079633 1.57079633 0.        ]\n",
      "Rotation_Matrix:[[ 0.  1.  0.]\n",
      " [ 0.  0. -1.]\n",
      " [-1.  0.  0.]]\n",
      "quaternion: [ 0.5  0.5  0.5 -0.5]\n",
      "Rotation_matrix_reobtained: [[ 0.  1.  0.]\n",
      " [ 0.  0. -1.]\n",
      " [-1.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "#2.1\n",
    "import numpy as np\n",
    "\n",
    "alpha = np.radians(90)\n",
    "beta = np.radians(90)\n",
    "gamma = np.radians(0) \n",
    "\n",
    "#method to calculate the rotation matrix given euler angles of rotation\n",
    "def R(angles):\n",
    "    phi = angles[0]\n",
    "    theta = angles[1]\n",
    "    psi = angles[2]\n",
    "    R_x = np.array( [ [ 1 , 0 , 0 ] ,\n",
    "                   [ 0 , np.cos(phi) , -np.sin(phi) ]  , \n",
    "                   [0 , np.sin(phi) , np.cos(phi)]])\n",
    "    \n",
    "    R_y = np.array( [ [ np.cos(theta) , 0 , np.sin(theta) ] ,\n",
    "                   [ 0 ,1 ,0 ] ,\n",
    "                   [ -np.sin(theta) , 0 , np.cos(theta)]])\n",
    "    R_z = np.array( [ [  np.cos(psi) , -np.sin(psi) , 0 ] , \n",
    "                   [np.sin(psi) , np.cos(psi) , 0 ] ,\n",
    "                   [0 ,0, 1]]  )\n",
    "    Rm = R_z@R_y@R_x\n",
    "\n",
    "    return Rm\n",
    "\n",
    "def R2Q(Rm):\n",
    "    M = np.array(Rm)\n",
    "    tr = np.trace(M)\n",
    "    if(tr > 0):\n",
    "        sq = np.sqrt(tr + 1)*2\n",
    "        qw = 0.25*sq\n",
    "        qx = (M[2][1] - M[1][2])/sq\n",
    "        qy = (M[0][2] - M[2][0])/sq\n",
    "        qz = (M[1][0] - M[0][1])/sq\n",
    "        \n",
    "    elif(M[0][0] > M[1][1] and M[0][0] > M[2][2]):\n",
    "        sq = np.sqrt(1 + M[0][0] - M[1][1] - M[2][2])*2\n",
    "        qw = (M[2][1] - M[1][2])/sq\n",
    "        qx = 0.25*sq\n",
    "        qy = (M[0][1] + M[1][0])/sq\n",
    "        qz = (M[0][2] + M[2][0])/sq\n",
    "        \n",
    "    elif(M[1][1] > M[2][2]):\n",
    "        sq = np.sqrt(1 + M[1][1] - M[0][0] - M[2][2])*2\n",
    "        qw = (M[0][2] - M[2][0])/sq\n",
    "        qx = (M[0][1] + M[1][0])/sq\n",
    "        qy = 0.25*sq\n",
    "        qz = (M[1][2] + M[2][1])/sq\n",
    "        \n",
    "    else:\n",
    "        sq = np.sqrt(1 + M[2][2] - M[0][0] - M[1][1])*2\n",
    "        qw = (M[1][0] - M[0][1])/sq\n",
    "        qx = (M[0][2] + M[2][0])/sq\n",
    "        qy = (M[1][2] + M[2][1])/sq\n",
    "        qz = 0.25*sq\n",
    "\n",
    "    q = np.array([qw,qx,qy,qz])\n",
    "    return q\n",
    "\n",
    "def Q2R(q):\n",
    "    norm_reciprocal = 1/np.sqrt(q[0]*q[0] + q[1]*q[1] + q[2]*q[2] + q[3]*q[3]) #Normalize\n",
    "    qw = q[0]*norm_reciprocal\n",
    "    qx = q[1]*norm_reciprocal\n",
    "    qy = q[2]*norm_reciprocal\n",
    "    qz = q[3]*norm_reciprocal\n",
    "    \n",
    "    Rm = np.array([[2*(qw*qw + qx * qx)-1, 2*(qx*qy - qw*qz), 2*(qx*qz + qw*qy)],\n",
    "                           [2*(qx*qy + qw*qz),  2*(qw*qw + qy*qy) -1, 2*(qy*qz - qw*qx)],\n",
    "                           [2*(qx*qz - qw*qy), 2*(qy*qz + qw*qx), 2*(qw*qw + qz*qz)-1]])\n",
    "                            \n",
    "    return Rm\n",
    "\n",
    "\n",
    "\n",
    "angles = np.array( [alpha, beta, gamma]) # X, Y, Z\n",
    "print(f\"The entered euler angles are[alpha,beta,gamma]: {angles}\")\n",
    "Rotation_Matrix=R(angles)\n",
    "print(f\"Rotation_Matrix:{Rotation_Matrix}\")\n",
    "\n",
    "q = R2Q(Rotation_Matrix)\n",
    "print(f\"quaternion: {q}\")\n",
    "\n",
    "Rotation_Matrix_Reobtained = Q2R(q)\n",
    "\n",
    "print(f\"Rotation_matrix_reobtained: {Rotation_Matrix_Reobtained}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb7aebf",
   "metadata": {},
   "source": [
    "It can be seen that the rotation matrix obtained from the euler angles and the rotation matrix obtained reobtained from the quaternions are equal and consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cc42b4",
   "metadata": {},
   "source": [
    "### Q2\n",
    "What is Gimbal lock?\n",
    "When two rotational axis are aligned with each other or parallel to each other, it is called a gimbal lock. In a gimbal lock, 1 DoF is lost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43e6907",
   "metadata": {},
   "source": [
    "### Q2.2 1st bullet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "520072a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rotation_Matrix:[[ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [-1.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "alpha = 0\n",
    "beta = np.radians(90)\n",
    "gamma = 0\n",
    "\n",
    "angles = np.array([alpha,beta,gamma])\n",
    "Rotation_Matrix=R(angles)\n",
    "print(f\"Rotation_Matrix:{Rotation_Matrix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275e97fc",
   "metadata": {},
   "source": [
    "### Q2.2 2nd bullet\n",
    "* There exists two gamma and infinitely many alpha, considering that range of rotation is [-pi, pi] rads around each axis, for which the value of Pnew = [1,0,0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e69fddc",
   "metadata": {
    "id": "0e69fddc"
   },
   "source": [
    "## 3. Transformations and Homogeneous Coordinates\n",
    "\n",
    "1. Watch this [video](https://www.youtube.com/watch?v=PvEl63t-opM) to briefly understand homogeneous coordinates. \n",
    "    1. What are points at infinity? What type of transformation can you apply to transform a point from infinity to a point that is not at infinity? \n",
    "    2. Find the vanishing point for the given images in the **data/Q3** folder. Complete function **FilterLines()** and  **GetVanishingPoint()** in the given starter code.\n",
    "\n",
    "<br>\n",
    "\n",
    "2. Using homogeneous coordinates we can represent different types of transformation as point transforms vs. frame transforms. Concatenation of transforms (whether you post multiply transformation matrices or pre-multiply transformation matrices) depends on the problem and how you are viewing it. Try to understand the difference between frame vs. point transformations from this [video](https://youtu.be/Za7Sdegf8m8?t=1834). We have 5 camera frames A, B, C, D and E. Given *frame* transformation $A \\rightarrow B$ ,  $B \\rightarrow C$ ,  $D \\rightarrow C$ ,  $D \\rightarrow E$. Compute *frame transformation*  $D \\rightarrow E$. Also, given the co-ordinates of a point *x* in *D's* frame, what transformation is required to get *x's*  co-ordinates in *E's* frame? \n",
    "\n",
    "    <img src=\"img/3.jpeg\"  width=\"500\" >\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3917ed0",
   "metadata": {
    "id": "b3917ed0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def ReadImage(InputImagePath):\n",
    "    Images = []                     # Input Images will be stored in this list.\n",
    "    ImageNames = []                 # Names of input images will be stored in this list.\n",
    "    \n",
    "    # Checking if path is of file or folder.\n",
    "    if os.path.isfile(InputImagePath):\t\t\t\t\t\t    # If path is of file.\n",
    "        InputImage = cv2.imread(InputImagePath)                 # Reading the image.\n",
    "        \n",
    "        # Checking if image is read.\n",
    "        if InputImage is None:\n",
    "            print(\"Image not read. Provide a correct path\")\n",
    "            exit()\n",
    "        \n",
    "        Images.append(InputImage)                               # Storing the image.\n",
    "        ImageNames.append(os.path.basename(InputImagePath))     # Storing the image's name.\n",
    "\n",
    "\t# If path is of a folder contaning images.\n",
    "    elif os.path.isdir(InputImagePath):\n",
    "\t\t# Getting all image's name present inside the folder.\n",
    "        for ImageName in os.listdir(InputImagePath):\n",
    "\t\t\t# Reading images one by one.\n",
    "            InputImage = cv2.imread(InputImagePath + \"/\" + ImageName)\n",
    "\t\t\t\n",
    "            Images.append(InputImage)\t\t\t\t\t\t\t# Storing images.\n",
    "            ImageNames.append(ImageName)                        # Storing image's names.\n",
    "        \n",
    "    # If it is neither file nor folder(Invalid Path).\n",
    "    else:\n",
    "        print(\"\\nEnter valid Image Path.\\n\")\n",
    "        exit()\n",
    "\n",
    "    return Images, ImageNames\n",
    "        \n",
    "def GetLines(Image):\n",
    "\n",
    "    GrayImage = cv2.cvtColor(Image, cv2.COLOR_BGR2GRAY)\n",
    "    BlurGrayImage = cv2.GaussianBlur(GrayImage, (5, 5), 1)\n",
    "    EdgeImage = cv2.Canny(BlurGrayImage, 40, 255)\n",
    "    Lines = cv2.HoughLinesP(EdgeImage, 1, np.pi / 180, 50, 10, 15)\n",
    "    if Lines is None:\n",
    "        print(\"No enough lines for vanishing point detection\")\n",
    "        exit(0)\n",
    "    \n",
    "    return Lines\n",
    "    \n",
    "def FilterLines(Lines):\n",
    "    fil_lines = []\n",
    "    for Line in Lines:\n",
    "        dx = Line[0][2] - Line[0][0]\n",
    "        if dx == 0:\n",
    "            dx = 0.000001\n",
    "        dy = Line[0][3] - Line[0][1]\n",
    "        slope = np.arctan2(dy, dx)\n",
    "        if np.abs(np.rad2deg(slope)) >= 2.0 and np.abs(np.rad2deg(slope)) <= 88.0:\n",
    "            fil_lines.append([Line[0][0], Line[0][1], Line[0][2], Line[0][3], dy/dx])\n",
    "\n",
    "    return fil_lines\n",
    "\n",
    "def GetVanishingPoint(FilteredLines):\n",
    "    iters = 1000\n",
    "    line_pts_og = FilteredLines\n",
    "    min_dist = np.inf\n",
    "    vp_opt = [300, 400]\n",
    "    for i in range(iters):\n",
    "        line_pts = copy.deepcopy(line_pts_og)\n",
    "        idx = random.sample(line_pts, 2)\n",
    "        line_pts.remove(idx[0])\n",
    "        line_pts.remove(idx[1])\n",
    "        l1 = idx[0]\n",
    "        l2 = idx[1]\n",
    "\n",
    "        dx = l1[2] - l1[0]\n",
    "        if dx == 0:\n",
    "            dx = 0.00001\n",
    "        dy = l1[3] - l1[1]\n",
    "        m1 = dy/dx\n",
    "        a1 = -m1\n",
    "        b1 = 1\n",
    "        c1 = l1[1] - m1*l1[0]\n",
    "\n",
    "        dx = l2[2] - l2[0]\n",
    "        if dx == 0:\n",
    "            dx = 0.00001\n",
    "        dy = l2[3] - l2[1]\n",
    "        m2 = dy/dx\n",
    "        a2 = -m2\n",
    "        b2 = 1\n",
    "        c2 = l2[1] -m2*l2[0]\n",
    "\n",
    "        ix, iy = np.abs((b1*c2 - b2*c1)/(a1*b2 - a2*b1)), np.abs((c1*a2 - c2*a1)/(a1*b2 - a2*b1))\n",
    "\n",
    "        pts = np.array(line_pts)\n",
    "\n",
    "        a_line = -pts[:, 4]\n",
    "        b_line = 1\n",
    "        c_line = pts[:, 1] - pts[:, 4]*pts[:, 0]\n",
    "        dist_from_ip = np.abs(a_line*pts[:, 0] + b_line*pts[:, 1] + c_line)/np.sqrt(a_line**2 + b_line**2)\n",
    "        min_idx = np.argmin(dist_from_ip)\n",
    "        dist = dist_from_ip[min_idx]\n",
    "        if dist<min_dist:\n",
    "            min_dist = dist\n",
    "            vp_opt = [ix, iy]\n",
    "    return vp_opt\n",
    "\n",
    "Images, ImageNames = ReadImage(\"data/Q3\")            # Reading all input images\n",
    "\n",
    "\n",
    "for i in range(len(Images)):\n",
    "    Image = Images[i]\n",
    "    Lines = GetLines(Image)\n",
    "    FilteredLines = FilterLines(Lines)\n",
    "    VanishingPoint = GetVanishingPoint(FilteredLines)\n",
    "\n",
    "    if VanishingPoint is None:\n",
    "        print(\"No Vanisihing points\")\n",
    "        continue\n",
    "    for Line in Lines:\n",
    "        cv2.line(Image, (Line[0], Line[1]), (Line[2], Line[3]), (0, 255, 0), 2)\n",
    "    cv2.circle(Image, (int(VanishingPoint[0]), int(VanishingPoint[1])), 10, (0, 0, 255), -1)\n",
    "\n",
    "    plt.imshow(Image)\n",
    "    plt.title(\"plot\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f08d84",
   "metadata": {
    "id": "e5f08d84"
   },
   "source": [
    "## 4. LiDAR and Registration\n",
    "\n",
    "Point clouds are a collection of points that represent a 3D shape or feature. Each point has its own set of X, Y and Z coordinates and in some cases additional attributes. A popular way to obtain this is by photogrammetry, though here we will use LiDAR data.\n",
    "\n",
    "LiDAR is a remote sensing process which collects measurements used to create 3D models and maps of objects and environments. Using ultraviolet, visible, or near-infrared light, LiDAR gauges spatial relationships and shapes by measuring the time it takes for signals to bounce off objects and return to the scanner.\n",
    "\n",
    "Download the data from [here](https://iiitaphyd-my.sharepoint.com/:f:/g/personal/venkata_surya_students_iiit_ac_in/EnYAMaTVIhJItzKYqtahE30BRKB6p6UfHN3TyJzvo6Mw0g?e=PegWds). It contains the LIDAR sensor output and odometry information per frame.\n",
    "\n",
    "  The .bin files contain the 3D point cloud captured by the LIDAR in this format - x, y, z, and reflectance. \n",
    "\n",
    "  The odometry information is given in the `odometry.txt` file, which is a 12 element vector. Reshape each of the first 77 rows to a 3x4 matrix to obtain the pose.\n",
    "    \n",
    "The point cloud obtained is with respect to the LiDAR frame. The poses however, are in the camera frame. If we want to combine the point clouds from various frames, we need to bring them to the camera frame. \n",
    "\n",
    "1. Refer to the image below and apply the required transformation to the point cloud. \n",
    "<br>\n",
    "\n",
    "    <img src=\"img/4.jpeg\"  width=\"500\" >\n",
    "\n",
    "<br>\n",
    "\n",
    "2. Then, register all point clouds into a common reference frame and visualise it (Open3D). It is helpful to use homogeneous coordinates to keep track of the different frames.\n",
    "\n",
    "3. Write a function to transform the registered point cloud from the world to the $i^{th}$ camera frame, wherein $i$ is the input to the function.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-termination",
   "metadata": {
    "id": "directed-termination"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MR 2022 A1.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "b671c20432fcd147198c92e7f072af9e705f087eb990bee22b07f08caab9f630"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
